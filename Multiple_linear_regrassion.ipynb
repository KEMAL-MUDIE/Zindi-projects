{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVW2iGV/rLmjCCmeVFPO/k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KEMAL-MUDIE/Zindi-projects/blob/main/Multiple_linear_regrassion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-smxZwHxQLx"
      },
      "outputs": [],
      "source": [
        "Multiple linear regrassion just for reference.\n",
        "# Example of solving multivariate linear regression in Python.\n",
        "#\n",
        "# Uses only Numpy, with Matplotlib for plotting.\n",
        "#\n",
        "# Eli Bendersky (http://eli.thegreenplace.net)\n",
        "# This code is in the public domain\n",
        "from __future__ import print_function\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from timer import Timer\n",
        "\n",
        "\n",
        "def read_CCPP_data(filename):\n",
        "    \"\"\"Read data from the given CCPP CSV file.\n",
        "    Returns (data, header). data is a 2D Numpy array of type np.float32, with a\n",
        "    sample per row. header is the names of the columns as read from the CSV\n",
        "    file.\n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as file:\n",
        "        reader = csv.reader(file)\n",
        "        header = reader.next()\n",
        "        return np.array(list(reader), dtype=np.float32), header\n",
        "\n",
        "\n",
        "def feature_normalize(X):\n",
        "    \"\"\"Normalize the feature matrix X.\n",
        "    Given a feature matrix X, where each row is a vector of features, normalizes\n",
        "    each feature. Returns (X_norm, mu, sigma) where mu and sigma are the mean\n",
        "    and stddev of features (vectors).\n",
        "    \"\"\"\n",
        "    num_features = X.shape[1]\n",
        "    mu = X.mean(axis=0)\n",
        "    sigma = X.std(axis=0)\n",
        "    X_norm = (X - mu) / sigma\n",
        "    return X_norm, mu, sigma\n",
        "\n",
        "\n",
        "def compute_cost(X, y, theta):\n",
        "    \"\"\"Compute the MSE cost of a prediction based on theta, over the whole X.\n",
        "    X: (k, n) each row is an input with n features (including an all-ones\n",
        "       column that should have been added beforehead).\n",
        "    y: (k, 1) observed output per input.\n",
        "    theta: (n, 1) regression parameters.\n",
        "    Note: expects y and theta to be proper column vectors.\n",
        "    \"\"\"\n",
        "    k = X.shape[0]\n",
        "    # Vectorized computation of yhat per sample.\n",
        "    yhat = np.dot(X, theta)\n",
        "    diff = yhat - y\n",
        "    # Vectorized computation using a dot product to compute sum of squares.\n",
        "    cost = np.dot(diff.T, diff) / k\n",
        "    # Cost is a 1x1 matrix, we need a scalar.\n",
        "    return cost.flat[0]\n",
        "\n",
        "\n",
        "def gradient_descent(X, y, nsteps, learning_rate=0.1):\n",
        "    \"\"\"Runs gradient descent optimization to fit a line y^ = theta.dot(x).\n",
        "    X: (k, n) each row is an input with n features (including an all-ones column\n",
        "       that should have been added beforehead).\n",
        "    y: (k, 1) observed output per input.\n",
        "    nsteps: how many steps to run the optimization for.\n",
        "    learning_rate: learning rate of gradient descent.\n",
        "    Yields 'nsteps + 1' pairs of (theta, cost) where theta is the fit parameter\n",
        "    shaped (n, 1) for that step, and its cost vs the real y. The first pair has\n",
        "    the initial theta and cost; the rest carry results after each of the\n",
        "    iteration steps.\n",
        "    \"\"\"\n",
        "    k, n = X.shape\n",
        "    theta = np.zeros((n, 1))\n",
        "    yield theta, compute_cost(X, y, theta)\n",
        "    for step in range(nsteps):\n",
        "        # yhat becomes a (k, 1) array of predictions, per sample.\n",
        "        yhat = np.dot(X, theta)\n",
        "        diff = yhat - y\n",
        "        dtheta = np.zeros((n, 1))\n",
        "        for j in range(n):\n",
        "            # The sum over all samples is computed with a dot product between\n",
        "            # (y^-y) and the jth feature across all of X.\n",
        "            dtheta[j, 0] = learning_rate * np.dot(diff.T, X[:, j]) / k\n",
        "        theta -= dtheta\n",
        "        yield theta, compute_cost(X, y, theta)\n",
        "\n",
        "\n",
        "def compute_normal_eqn(X, y):\n",
        "    \"\"\"Compute theta using the normal equation.\n",
        "    X is the input matrix with a leftmost column of 1s. Returns theta as a\n",
        "    column vector.\n",
        "    \"\"\"\n",
        "    XTX = np.dot(X.T, X)\n",
        "    # Using linalg.inv here, which will bomb for a singular matrix.\n",
        "    # Alternatively, we could use linalg.pinv to compute a pseudo-inverse.\n",
        "    XTX_inv = np.linalg.inv(XTX)\n",
        "    xdot = np.dot(XTX_inv, X.T)\n",
        "    return np.dot(xdot, y)\n",
        "\n",
        "\n",
        "def split_dataset_to_train_test(dataset, train_proportion=0.8):\n",
        "    \"\"\"Splits the dataset to a train set and test set.\n",
        "    The split is done over a random shuffle of the rows of the dataset. Assumes\n",
        "    each row in the data has the expected outcome in the last column.\n",
        "    train_proportion:\n",
        "        The proportion of data to keep in the training set. The rest goes to the\n",
        "        test set.\n",
        "    Returns (X_train, y_train, X_test, y_test) where y_train/y_test are column\n",
        "    vectors taken from the last column of the dataset.\n",
        "    \"\"\"\n",
        "    shuffled_dataset = np.random.permutation(dataset)\n",
        "    k_train = int(shuffled_dataset.shape[0] * train_proportion)\n",
        "\n",
        "    X_train = shuffled_dataset[:k_train, :-1]\n",
        "    y_train = shuffled_dataset[:k_train, -1].reshape(-1, 1)\n",
        "    X_test = shuffled_dataset[k_train:, :-1]\n",
        "    y_test = shuffled_dataset[k_train:, -1].reshape(-1, 1)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def compute_rsquared(X, y, theta):\n",
        "    \"\"\"Compute R^2 - the coefficeint of determination for theta.\n",
        "    X: (k, n) input.\n",
        "    y: (k, 1) observed output per input.\n",
        "    theta: (n, 1) regression parameters.\n",
        "    Returns the R2 - a scalar.\n",
        "    \"\"\"\n",
        "    k = X.shape[0]\n",
        "    yhat = np.dot(X, theta)\n",
        "    diff = yhat - y\n",
        "    SE_line = np.dot(diff.T, diff)\n",
        "    SE_y = len(y) * y.var()\n",
        "    return (1 - SE_line / SE_y).flat[0]\n",
        "\n",
        "\n",
        "def plot_cost_vs_step(costs):\n",
        "    \"\"\"Given an array of costs, plots them vs. index.\n",
        "    Uses logarithmic scale for y be cause the cost tends to be very large\n",
        "    initially.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(range(len(costs)), costs)\n",
        "    ax.set_yscale('log')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_correlation_heatmap(X, header):\n",
        "    \"\"\"Plot a heatmap of the correlation matrix for X.\n",
        "    This requires the seaborn package to be installed.\n",
        "    \"\"\"\n",
        "    import seaborn\n",
        "    cm = np.corrcoef(X.T)\n",
        "    hm = seaborn.heatmap(cm,\n",
        "            cbar=True,\n",
        "            annot=True,\n",
        "            square=True,\n",
        "            yticklabels=header,\n",
        "            xticklabels=header)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def sample_predictions_vs_truth(X, y, theta, nsamples=10):\n",
        "    \"\"\"Display a sample of predictions vs. true values.\"\"\"\n",
        "    print('Sample of predictions vs. true values')\n",
        "    yhat = np.dot(X, theta)\n",
        "    sample_indices = np.random.choice(X.shape[0], size=nsamples, replace=False)\n",
        "    for index in sample_indices:\n",
        "        print('  sample #{0}: yhat={1}, y={2}'.format(index,\n",
        "                                                      yhat[index][0],\n",
        "                                                      y[index][0]))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Follow through the code here to see how the functions are used. No\n",
        "    # plotting is done by default. Uncomment relevant lines to produce plots.\n",
        "\n",
        "    # For reproducibility\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # This file was dowloaded from:\n",
        "    # https://archive.ics.uci.edu/ml/machine-learning-databases/00294/ and then\n",
        "    # unzipped.\n",
        "    filename = 'CCPP-dataset/data.csv'\n",
        "    with Timer('reading data'):\n",
        "        X, header = read_CCPP_data(filename)\n",
        "\n",
        "    # Plot a heatmap for the correlation matrix of X. This requires the seaborn\n",
        "    # package. This heatmap is a useful visualization for finding features that\n",
        "    # are most correlated with the result, and features that are possibly\n",
        "    # collinear.\n",
        "    #plot_correlation_heatmap(X, header)\n",
        "\n",
        "    print('Read {0} data samples from {1}'.format(len(X), filename))\n",
        "    X_train, y_train, X_test, y_test = split_dataset_to_train_test(X)\n",
        "    print('Data shapes:')\n",
        "    print('  X_train:', X_train.shape)\n",
        "    print('  y_train:', y_train.shape)\n",
        "    print('  X_test:', X_test.shape)\n",
        "    print('  y_test:', y_test.shape)\n",
        "\n",
        "    # Normalize X to bring all features into the same scale. Also, add a\n",
        "    # all-ones column as the first column of X (\"augmented X\") to serve as the\n",
        "    # bias term.\n",
        "    ktrain = X_train.shape[0]\n",
        "    X_train_normalized, mu, sigma = feature_normalize(X_train)\n",
        "    X_train_augmented = np.hstack((np.ones((ktrain, 1)), X_train_normalized))\n",
        "\n",
        "    # Run gradient descent.\n",
        "    NSTEPS = 500\n",
        "    with Timer('Running gradient descent ({0} steps)'.format(NSTEPS)):\n",
        "        thetas_and_costs = list(gradient_descent(X_train_augmented,\n",
        "                                                 y_train, NSTEPS))\n",
        "    # Plot cost vs. step for the last 100 steps (the first steps have an\n",
        "    # enourmous errors compared to the final steps).\n",
        "    #plot_cost_vs_step([cost for _, cost in thetas_and_costs][:100])\n",
        "\n",
        "    last_theta = thetas_and_costs[-1][0]\n",
        "    print('Best theta found:', last_theta)\n",
        "\n",
        "    print('Training set MSE:',\n",
        "          compute_cost(X_train_augmented, y_train, last_theta))\n",
        "    print('Training set R^2:',\n",
        "          compute_rsquared(X_train_augmented, y_train, last_theta))\n",
        "\n",
        "    # Normalize the test set using the mu/sigma computed from the training set,\n",
        "    # and augment it with the bias column of 1s.\n",
        "    ktest = X_test.shape[0]\n",
        "    X_test_normalized = (X_test - mu) / sigma\n",
        "    X_test_augmented = np.hstack((np.ones((ktest, 1)), X_test_normalized))\n",
        "    print('Test set MSE:',\n",
        "          compute_cost(X_test_augmented, y_test, last_theta))\n",
        "    print('Test set R^2:',\n",
        "          compute_rsquared(X_test_augmented, y_test, last_theta))\n",
        "\n",
        "    # To assess how good the fit is, print out a random sample of predictions\n",
        "    # for the test set compared to the real y values for these inputs.\n",
        "    sample_predictions_vs_truth(X_test_augmented, y_test, last_theta)\n",
        "\n",
        "    # Compute theta using the normal equation and report MST / R^2.\n",
        "    theta_from_normal_eqn = compute_normal_eqn(X_train_augmented, y_train)\n",
        "    print('Theta from normal equation:', theta_from_normal_eqn)\n",
        "    print('Test set MSE / normal:',\n",
        "          compute_cost(X_test_augmented, y_test, theta_from_normal_eqn))\n",
        "    print('Test set R^2 / normal:',\n",
        "          compute_rsquared(X_test_augmented, y_test, theta_from_normal_eqn))"
      ]
    }
  ]
}